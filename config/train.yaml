world_size: 4
experiment_name: 'FFHQH_training'
datasets:
    iharmony: "/home/jovyan/datasets/iHarmony4/"
    crop_size: 1024
    # dataset_root: "/home/jovyan/datasets/iHarmony4/"
    ffhqh: "/home/jovyan/datasets/ffhqh/"
    dataset_root: "/home/jovyan/datasets/ffhqh/"
    ffhq: "/home/jovyan/datasets/ffhq/images1024x1024/"
task_for_oneformer: "matting"
datasets_to_use_test: ["FFHQH"]
datasets_to_use_val: ["FFHQH"]
dataset_to_use_train: "FFHQH"
pretrained_model: 'microsoft/swin-base-patch4-window7-224' #"microsoft/swinv2-tiny-patch4-window8-256"  #"nielsr/mask2former-swin-base-youtubevis-2021" #"nvidia/mit-b2"
batch_size: 4
num_workers: 4
log_dir: "log"
checkpoint_dir: "checkpoints"
load_pretrained: False
checkpoint: "best.pth"
distributed_addr: "localhost"
lr: 1e-5
epochs: 300
disable_validation: False
disable_mixed_precision: False
log_image_interval: 10
log_image_number: 4
save_model_interval: 5
lambda_losses: 
    default: 0.
    Color: 0.
    PSNR: 2e-1
    FnMSE: 1e-2
    LPIPS: 0.
    Gradient: 1e-2
    L2: 0.
    L1: 0.
    softmax: 1e-4
model:
    grid_counts: [4, 3, 2, 1]
    enc_sizes: [3, 16, 64, 128, 256, 512, 1024]
    init_weights: [0.5, 0.5] # weights of local grided image and global harmonized
    init_value: 0.8
    skips: True
